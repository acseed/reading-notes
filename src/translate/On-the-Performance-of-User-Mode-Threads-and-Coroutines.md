# On the Performance of User-Mode Threads and Coroutines


关于用户态线程和协程（如Project Loom的虚拟线程或Go的goroutines）的讨论，经常会涉及到性能问题。我在这里尝试回答的问题是，用户态线程是如何在性能上超越操作系统线程的？

一个常见的假设是，这与任务切换成本有关，用户态线程和协程的性能优势（就本次讨论的相关方面而言，两者相似，因此我将交替使用这两个术语）在于，与操作系统线程相比，它们的任务切换开销较低。因此，一些人进一步询问，与异步编程（使用回调或某些异步组合，例如CompletableFuture或Flow/reactive streams提供的方式）相比，用户态线程在任务切换开销上增加了多少，以及任务切换成本是与异步编程相比损害了性能还是提升了性能。但正如我们将看到的，在它们最常见、最有用的用例中，用户态线程（或协程）的性能优势与任务切换成本关系不大。它们的力量来自于其他方面。

性能是一个复杂的话题，所以首先，让我们来定义几个术语。**吞吐量**是一种性能度量，定义为单位时间内完成的操作数量；**延迟**是另一种性能度量，是指完成某项操作所需的时间。例如，将数据从伦敦传到纽约，通过将数据放在闪存驱动器上，然后通过747飞机运输，这将遭受很高的延迟，但可以拥有相当高的吞吐量。

另一个重要的概念是我将称之为“**影响**”。它是某个操作对整体性能的贡献，是衡量优化该操作重要性的一个指标。如果某个操作占用了应用程序总时间的1%，那么即使将该操作的工作速度提高无限倍，也只会将应用程序的总性能提高1%，因此这个操作的影响就很低。影响高度依赖于应用程序，但我们将讨论常见类型的应用程序，并通过用例来分类影响。

考虑协程的一个用例：生成器——编写迭代器的便捷方式。假设一个生成器向一个消费者发出递增整数序列；在每次产生数字时，生成器和消费者之间会发生任务切换。关键的是，这个场景除了纯粹的运算之外什么都没有，而且非常短暂。我们将这个称为“纯计算”用例。吞吐量是每秒累加的整数数量。在这种情况下，任务切换操作的影响非常高。如果我们假设处理时间（递增数字和求和）为零，那么任务切换开销的影响就是100%。

现在考虑另一个我们将称之为“事务处理”的用例：服务器等待并响应通过网络到达的请求。当一个请求到达时，服务器通过进行一些计算以及通过网络联系其他辅助服务（例如数据库）来处理它；它向这些其他服务发送请求，收集它们的响应，并最终向客户端回复一些汇总结果。这个系统的吞吐量是服务器每秒处理的传入请求数量。

我们的服务器如果请求不会无限累积，就可以被认为是稳定的，因此响应率等于请求率。为了分析其吞吐量，我们转向Little’s law，该定律指出，在稳定系统中，平均并发水平L——服务器并发处理的请求数量——等于平均请求率λ乘以处理每个请求的平均持续时间W： 

```
L = λW
```

因为系统是稳定的，其吞吐量等于λ，可达到的最大λ是系统的容量。这个定律很简单，但实际上非常了不起，因为结果并不依赖于请求到达的分布。

为了简化问题，我们假设，如前所述，所有计算耗时为零，只考虑联系辅助服务的网络延迟时间，因为我们期望这个成本会主导延迟W。有两个细微之处需要处理：首先，如果我们的服务器采用多个可以并行处理的核心，我们可以将每个核心视为一个独立的服务器；因此，在不失一般性的情况下，我们假设我们的服务器是单核心的（确实，核心共享网络接口，但我们将忽略这个复杂性）。其次，如果在处理请求的过程中，我们必须使用三个网络服务，每个服务响应时间为1毫秒，我们可以通过并行执行它们将收集它们响应的总延迟从3毫秒减少到1毫秒。这会将W减少三倍，但与远程服务的交互也会增加三个额外的飞行中的子操作，将我们的并发水平L也增加三倍，两者相互抵消。因此，为了应用Little's law，我们应该将W视为所有外出请求持续时间的总和。因此，我们的吞吐量是：
```
λ = L/W
```
我们现在准备计算任务切换的影响。W是我们服务请求的延迟总和，因为我们在讨论平均值，所以它是每个事务的平均服务调用次数n乘以平均服务调用延迟w，所以
```
W = nw。
```
在单个核心上同时处理多个请求时，当我们等待服务的响应时，我们会切换到另一个事务，因此每个外出调用都伴随着一个任务切换。如果平均任务切换延迟是t，等待服务响应的平均等待时间是μ，那么w = µ + t，W = n(µ + t)。任务切换t对吞吐量λ的影响——如果我们假设任务切换成本为零，我们会获得多少容量——那么，

```
(L/n(µ + 0)) / (L/n(µ + t)) = n(µ + t)/nµ = (µ + t)/µ = 1 + t/µ
```

如果我们选择在每个操作系统线程上处理每个事务，并且每次等待服务时都阻塞，通过内核的任务切换——一个可能需要t = 1微秒的慢操作——即使我们的服务和网络非常快，给我们一个平均服务调用延迟μ为20微秒，任务切换的影响也是1/20。通过优化任务切换，我们希望能将容量提高的最佳情况是5%！更一般地说，任务切换的影响是其平均延迟除以平均等待时间。如果涉及到等待网络IO，那么即使任务切换相对低效，这个比率也可能相当低。

显然，要显著提高这类系统的容量，我们不应该专注于降低t，这只会适度减少延迟W并增加吞吐量λ，而应该专注于增加L，我们可以并发处理的事务数量。如果我们保持简单的每个请求一个线程的编程模型，并在单个线程上处理每个事务，L将是我们需要的活动线程数量。这就是用户态线程如何提供帮助：它们通过数量级的增加L，可能有数百万个用户态线程，而不是操作系统可以支持的一点点数千个（但不要期望容量增加1000倍；我们忽略了计算成本，并且肯定会在辅助服务中遇到瓶颈）。异步编程也以相同的方式提高性能：不是通过减少任务切换成本，而是通过增加L，并发处理的事务数量，只是它不使用线程来表示每个事务，而是使用不同的结构。

尽管如此，任务切换开销仍然很重要。如果N是每个事务的任务切换次数，那么W = nµ + Nt，任务切换对吞吐量的影响实际上是：

```
Nt / nµ
```
我们假设每个服务调用一次任务切换，所以N = n，但当我们有如此多的线程可供使用时，通过传递消息相互通信，使用几个线程来处理事务是很方便的。这可能会将每个事务的任务切换次数远远超过每个服务调用一次，因此保持任务切换成本低仍然是一个好主意。这很重要，只是没有我们天真地认为的那么重要。

我们相信，在Java中，事务处理是用户态线程或协程比纯计算用例更重要的用例，因此，虽然Loom努力保持虚拟线程的任务切换成本低，但这并不是其对性能的主要贡献，也不是其力量的主要来源。如果针对事务处理用例的便利性与任务切换开销之间出现冲突，我们优先考虑前者，原因我希望现在已经很清楚了。

最后，关于实现的一些话：几乎不可能从一种语言中的协程或用户态线程的实现质量推断出另一种语言。一些语言允许指向局部变量的指针，而其他语言则不允许。在一些语言中，分配是廉价的并且可以隐藏，而在其他语言中可能代价高昂/或需要显式管理。正如我们所见，目标用例可能不同。一个设计，例如，优化所有参与某些工作的协程可能适合CPU缓存的情况（如生成器）可能不适合涉及大量任务且任务切换不可避免地导致缓存未命中的用例。判断一个实现的优点需要在其试图优化的用例的背景下评估它，以及它针对的语言/运行时的特有约束和优势。但这又是另一个讨论的主题。
